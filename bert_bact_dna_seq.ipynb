{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Ud4emk_-v_NZ",
        "YyERJ1CpwIwy",
        "gGhX01Tb3Oew",
        "rKdMWyKU-iU_"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "48db4b9e6211487d93369fcd18a2b5d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ad8b37bf3c94080ab389682904a1fe4",
              "IPY_MODEL_cf1e8c18d7ad430aab3199a3843c0da8",
              "IPY_MODEL_3e0e345f15a441498744edebaee0daf1"
            ],
            "layout": "IPY_MODEL_f9074e279d5644dba800ad576b8d57f2"
          }
        },
        "6ad8b37bf3c94080ab389682904a1fe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8fccfd9df4e48f584ccf4889e37f768",
            "placeholder": "​",
            "style": "IPY_MODEL_bb022b8355a64c8982ba2ac1ad64d586",
            "value": "100%"
          }
        },
        "cf1e8c18d7ad430aab3199a3843c0da8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2a4b164348143e1a9b7f9405999eed4",
            "max": 22707,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_702b43def5f24adab9b9e3b3e8dd50da",
            "value": 22707
          }
        },
        "3e0e345f15a441498744edebaee0daf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_342afc0d69ac47748aeae044e0b20f6b",
            "placeholder": "​",
            "style": "IPY_MODEL_c74fa26aa4c348d49646b2d9a3af1e07",
            "value": " 22707/22707 [00:01&lt;00:00, 15341.98it/s]"
          }
        },
        "f9074e279d5644dba800ad576b8d57f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8fccfd9df4e48f584ccf4889e37f768": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb022b8355a64c8982ba2ac1ad64d586": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2a4b164348143e1a9b7f9405999eed4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "702b43def5f24adab9b9e3b3e8dd50da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "342afc0d69ac47748aeae044e0b20f6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c74fa26aa4c348d49646b2d9a3af1e07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "796be56d30fc45e7b92a888a9ed23616": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_27cc742f50ed4c76a294b19bd84aa199",
              "IPY_MODEL_bcef4b0ac0db442fa9da5b7a12b44660",
              "IPY_MODEL_e3accf0bbd574238b6ad286804f75c56"
            ],
            "layout": "IPY_MODEL_66da9b4d961f48cd8965b3450c51ed64"
          }
        },
        "27cc742f50ed4c76a294b19bd84aa199": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d0467d8fe074952916461dbf0cb9905",
            "placeholder": "​",
            "style": "IPY_MODEL_f0afd2a523ec4d57840f898133512271",
            "value": "100%"
          }
        },
        "bcef4b0ac0db442fa9da5b7a12b44660": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5976801351b45349ea8c356ee76fd0c",
            "max": 18909,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d3e97a97379e4f1d86fc64e4fc064047",
            "value": 18909
          }
        },
        "e3accf0bbd574238b6ad286804f75c56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67981d12c63a4dd2a7a2a13ff7fc8236",
            "placeholder": "​",
            "style": "IPY_MODEL_14e01489975a4f998f843f43639ce2dc",
            "value": " 18909/18909 [00:00&lt;00:00, 29041.52it/s]"
          }
        },
        "66da9b4d961f48cd8965b3450c51ed64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d0467d8fe074952916461dbf0cb9905": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0afd2a523ec4d57840f898133512271": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5976801351b45349ea8c356ee76fd0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3e97a97379e4f1d86fc64e4fc064047": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "67981d12c63a4dd2a7a2a13ff7fc8236": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14e01489975a4f998f843f43639ce2dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1225b02cb11e4201b5fd2999aeb29775": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e58fbe6e0fd4a638a0da84899912804",
              "IPY_MODEL_b89faa3d21594ff1ad8dae70940827a9",
              "IPY_MODEL_48c1ad030a0e4dfda1f1a09024a2dae2"
            ],
            "layout": "IPY_MODEL_bf00842609e34aeaa55f8473afd42ab7"
          }
        },
        "4e58fbe6e0fd4a638a0da84899912804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c253d729f94f4c5d89b57b648fb99f32",
            "placeholder": "​",
            "style": "IPY_MODEL_39fd83db939141c4b79949d9b1abd9c9",
            "value": "Epoch 0: 100%"
          }
        },
        "b89faa3d21594ff1ad8dae70940827a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7b9c85f667f4363a4a115be4716f9c7",
            "max": 2601,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9764d39196c8485b970211bffd8453bc",
            "value": 2601
          }
        },
        "48c1ad030a0e4dfda1f1a09024a2dae2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d099a92b61e4569ac8b53b804f03338",
            "placeholder": "​",
            "style": "IPY_MODEL_78e6744365b349d8a1ca3937bcec595b",
            "value": " 2601/2601 [41:17&lt;00:00,  1.05it/s, loss=0.286]"
          }
        },
        "bf00842609e34aeaa55f8473afd42ab7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c253d729f94f4c5d89b57b648fb99f32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39fd83db939141c4b79949d9b1abd9c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7b9c85f667f4363a4a115be4716f9c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9764d39196c8485b970211bffd8453bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d099a92b61e4569ac8b53b804f03338": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78e6744365b349d8a1ca3937bcec595b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc079af96e3b45108cc563816c66cba1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f5e25b6dcc64c56875821be575d2616",
              "IPY_MODEL_e2baaae531fb43548a6dbc03a047247d",
              "IPY_MODEL_72af49f2b24f4dea8b944b52b4924a92"
            ],
            "layout": "IPY_MODEL_371bb1f82fc145cda04f19e1501bf157"
          }
        },
        "7f5e25b6dcc64c56875821be575d2616": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9f8b6f25c9d4656a6e5d33e5cf98cd0",
            "placeholder": "​",
            "style": "IPY_MODEL_a0ea341535ef423a8acdf840851f6b84",
            "value": "Epoch 1: 100%"
          }
        },
        "e2baaae531fb43548a6dbc03a047247d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84c2442348d34039be134cf4c1026e15",
            "max": 2601,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_825fc53f71924cf49347110f5c9ce7f1",
            "value": 2601
          }
        },
        "72af49f2b24f4dea8b944b52b4924a92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f8d458c5c4543a690dc7809ed758e92",
            "placeholder": "​",
            "style": "IPY_MODEL_cc394a7ede2543e1aa6cabc71ff55a59",
            "value": " 2601/2601 [41:22&lt;00:00,  1.05it/s, loss=0.116]"
          }
        },
        "371bb1f82fc145cda04f19e1501bf157": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9f8b6f25c9d4656a6e5d33e5cf98cd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0ea341535ef423a8acdf840851f6b84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84c2442348d34039be134cf4c1026e15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "825fc53f71924cf49347110f5c9ce7f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f8d458c5c4543a690dc7809ed758e92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc394a7ede2543e1aa6cabc71ff55a59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install tensorboard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuGU2WOAqRqU",
        "outputId": "799e4b63-182e-4d0e-f567-be212d575733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.9/dist-packages (2.11.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (63.4.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (3.4.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (2.2.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (2.27.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (1.51.3)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (1.22.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (0.4.6)\n",
            "Requirement already satisfied: protobuf<4,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (3.19.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (2.16.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard) (6.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpxoCOM-neq3",
        "outputId": "031b4fbd-09f8-445d-c831-ed9164d143c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Mar 17 21:40:29 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    25W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Fri Mar 17 21:40:30 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    26W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "lqTYyX3ezqV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize google drive"
      ],
      "metadata": {
        "id": "rKmJ_mGCvtys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6tDEavSAvn7l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z1WCJQItdnU",
        "outputId": "218693b0-7843-459d-efcc-464f969b0c08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = \"/content/gdrive/My Drive/dna_seq/\""
      ],
      "metadata": {
        "id": "rcJmBnrbttJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Preprocessing"
      ],
      "metadata": {
        "id": "o2QmXpAft9oG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from tqdm.auto import tqdm\n",
        "\n",
        "# data_non = f\"/{root_dir}/cleaned_non_patho_data.txt\"\n",
        "# non_patho = pd.read_csv(data_non, sep=';', header=None)\n",
        "\n",
        "# data_patho = f\"/{root_dir}/cleaned_patho_data.txt\"\n",
        "# patho = pd.read_csv(data_patho, sep=';', header=None)\n",
        "\n",
        "# def write_spaced_data(df ,path_clean):\n",
        "#   for i in tqdm(range(0,len(df[0]))):\n",
        "#     name = df[0][i]\n",
        "#     data = split_dna_into_4letter_words(df[1][i],' ')\n",
        "#     data_content = f\"{name};{data}\\n\"\n",
        "#     path_clean.write(data_content)\n",
        "\n",
        "# path_clean = open(f\"/{root_dir}/cleaned_non_patho_data_word.txt\",'w')\n",
        "# write_spaced_data(non_patho,path_clean)\n",
        "\n",
        "# path_clean = open(f\"/{root_dir}/cleaned_patho_data_word.txt\",'w')\n",
        "# write_spaced_data(patho,path_clean)\n"
      ],
      "metadata": {
        "id": "Dj27VawweGFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get dataframe for patho and non-patho"
      ],
      "metadata": {
        "id": "Ko8rrOsMv2ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = f\"/{root_dir}/cleaned_non_patho_data_word.txt\"\n",
        "non_patho = pd.read_csv(data, sep=';', header=None)\n",
        "\n",
        "data = f\"/{root_dir}/cleaned_patho_data_word.txt\"\n",
        "patho = pd.read_csv(data, sep=';', header=None)\n"
      ],
      "metadata": {
        "id": "GCbfxnXIvH-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create corpus for dna seq with 4 letter word each (Do not run)\n"
      ],
      "metadata": {
        "id": "Ud4emk_-v_NZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dna_into_4letter_words(current_seq,split_delimiter):\n",
        "  seq_len = len(current_seq)\n",
        "  sample = []\n",
        "  for i in range(0,seq_len,4):\n",
        "    w = current_seq[i:i+4]\n",
        "    sample.append(w)\n",
        "  return split_delimiter.join(sample)\n"
      ],
      "metadata": {
        "id": "nU7L3gTbkl98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_patho[1][17999].replace(' ', '\\n') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "IjvncrBAlZhz",
        "outputId": "c6434a2e-f222-4b81-c309-fc2a95bff497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'gtgg\\nggtt\\nggcg\\nggtg\\nctat\\nacat\\ngcaa\\ngtcg\\naacg\\naact\\nctgg\\ntatt\\ngatt\\nggtg\\ncttg\\ncatc\\natga\\nttta\\ncatt\\ntgag\\ntgag\\ntggc\\ngaac\\ntggt\\ngagt\\naaca\\ncgtg\\nggaa\\nacct\\ngccc\\nagaa\\ngcgg\\nggga\\ntaac\\nacct\\nggaa\\nacag\\natgc\\ntaat\\naccg\\ncata\\nacaa\\ncttg\\ngacc\\ngcat\\nggtc\\ncgag\\ntttg\\naaag\\natgg\\ncttc\\nggct\\natca\\ncttt\\ntgga\\ntggt\\ncccg\\ncggc\\ngtat\\ntagc\\ntaga\\ntggt\\ngagg\\ntaac\\nggct\\ncacc\\natgg\\ncaat\\ngata\\ncgta\\ngccg\\nacct\\ngaga\\ngggt\\naatc\\nggcc\\nacat\\ntggg\\nactg\\nagac\\nacgg\\nccca\\naact\\nccta\\ncggg\\naggc\\nagca\\ngtag\\nggaa\\ntctt\\nccac\\naatg\\ngacg\\naaag\\ntctg\\natgg\\nagca\\nacgc\\ncgcg\\ntgag\\ntgaa\\ngaag\\nggtt\\ntcgg\\nctcg\\ntaaa\\nactc\\ntgtt\\ngtta\\naaga\\nagaa\\ncata\\ntctg\\nagag\\ntaac\\ntgtt\\ncagg\\ntatt\\ngacg\\ngtat\\nttaa\\nccag\\naaag\\nccac\\nggct\\naact\\nacgt\\ngcca\\ngcag\\nccgc\\nggta\\natac\\ngtag\\ngtgg\\ncaag\\ncgtt\\ngtcc\\nggat\\nttat\\ntggg\\ncgta\\naagc\\ngagc\\ngcag\\ngcgg\\ntttt\\nttaa\\ngtct\\ngatg\\ntgaa\\nagcc\\ntttc\\nggct\\ncaac\\ncgaa\\ngaag\\ntgca\\ntcgg\\naaac\\ntggg\\naaac\\nttga\\ngtgc\\nagaa\\naagg\\nacag\\ntgga\\nactc\\ncatg\\ntggt\\nagcg\\ngtga\\naatg\\ncgta\\ngaca\\ntatg\\ngaat\\naacc\\ntcag\\nggtc\\ngtag\\ncnnn\\nnnnn\\nnnnn\\nnnnn\\nnnnn\\nnnnn\\nnnnn\\nnnnn\\nnnnn\\nnnnn\\nnnnn\\nnnnn\\nnnnn\\nnnnn\\nnnnn\\nnnnn\\nnnnn\\nnnnn\\nnnnn\\nnnnn\\nnnnn\\nnnnn\\nnnnn\\nnnnn\\nnnnn\\nnagt\\naagt\\natat\\ntttc\\nacac\\nattg\\ngacg\\naatt\\nctag\\ntgag\\ncaac\\ngccc\\ngcgg\\naagg\\nagag\\nggtt\\ntcgg\\nctcg\\ntaaa\\ncttc\\ngttg\\ntaaa\\ngaag\\naacc\\naatc\\ntgag\\ngata\\nctgt\\nccag\\ngtat\\ntgac\\ncgta\\nttta\\nacca\\ngaaa\\ngcca\\ncggc\\ntaac\\ntacg\\ntgcc\\nagca\\ngccg\\ncggt\\naata\\ncgta\\nggtg\\ngcaa\\ngcgt\\ntgtc\\ncgga\\nttta\\ntggg\\ngcgt\\naaag\\ncgag\\ncgca\\nggcg\\ngttt\\nttaa\\ngtct\\ngatg\\ntgaa\\nagcc\\nttcg\\ngctc\\naacc\\ngaag\\naaag\\ntgca\\ntcgg\\naaac\\ntggg\\naaac\\nttga\\ngtgc\\nagaa\\ngagg\\nacag\\ntgga\\nactc\\ncatg\\ntgta\\ngcgg\\ntgaa\\natgc\\ngtag\\natat\\natgg\\naaga\\nacac\\ncagt\\nggcg\\naagg\\ncggc\\ntgtc\\ntggt\\nctgt\\naact\\ngacg\\nctga\\nggct\\ncgaa\\nagta\\ntggg\\ntagc\\naaac\\nagga\\nttag\\natac\\ncctg\\ngtag\\ntcca\\ntacc\\ngtaa\\nacga\\ntgaa\\ntgct\\naagt\\ngttg\\ngagg\\ngttt\\nccgc\\ncctt\\ncagt\\ngctg\\ncagc\\ntaac\\ngcat\\ntaag\\ncatt\\nccgc\\nctgg\\nggag\\ntacg\\ngccg\\ncaag\\ngctg\\naaac\\ntcaa\\nagga\\nattg\\nacgg\\ngggc\\nccgc\\nacaa\\ngcgg\\ntgga\\ngcat\\ngtgg\\nttta\\nattc\\ngaag\\nctac\\ngcga\\nagaa\\ncctt\\nacca\\nggtc\\nttga\\ncata\\nctat\\ngcaa\\natct\\naaga\\ngatt\\nagac\\ngttc\\ncctt\\ncggg\\ngaca\\ntgga\\ntaca\\nggtg\\ngtgc\\natgg\\nttgt\\ncgtc\\nagct\\ncgtg\\ntcgt\\ngaga\\ntgtt\\ngggt\\ntaag\\ntccc\\ngcaa\\ncgag\\ncgca\\naccc\\nttat\\ntatc\\nagtt\\ngcca\\ngcat\\ntaag\\nttgg\\ngcac\\ntctg\\ngtga\\ngact\\ngccg\\ngtga\\ncaaa\\nccgg\\nagga\\naggt\\ngggg\\natga\\ncgtc\\naaat\\ncatc\\natgc\\nccct\\ntatg\\nacct\\ngggc\\ntaca\\ncacg\\ntgct\\nacaa\\ntgga\\ntggt\\nacaa\\ncgag\\nttgc\\ngaac\\ntcgc\\ngaga\\ngtaa\\ngcta\\natct\\nctta\\naagc\\ncatt\\nctca\\ngttc\\nggat\\ntgta\\nggct\\ngcaa\\nctcg\\nccta\\ncatg\\naagt\\ncgga\\natcg\\nctag\\ntaat\\ncgcg\\ngatc\\nagca\\ntgcc\\ngcgg\\ntgaa\\ntacg\\nttcc\\ncggg\\ncctt\\ngtac\\nacac\\ncgcc\\ncgtc\\nacac\\ncatg\\nagag\\ntttg\\ntaac\\naccc\\naaag\\ntcgg\\ntggg\\ngtaa\\ncctt\\nttag\\ngaac\\ncagc\\ncgcc\\ntaag\\ngtgg\\ngaca\\ngatg\\natta\\ngggt\\ngaag\\ntcgt\\naaca\\nagag\\nccta\\ngtcc\\nc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## create corpus of dna with 4letter word size\n",
        "from tqdm.auto import tqdm\n",
        "def create_corpus(data):\n",
        "  text_data = []\n",
        "  file_count = 0\n",
        "  for i in tqdm(range(0,len(data[0]))):\n",
        "    sample = data[1][i]\n",
        "    # sample = data[1][i].replace(' ', '\\n')\n",
        "    text_data.append(sample)\n",
        "    if len(text_data) == 10_000:\n",
        "      # once we git the 10K mark, save to file\n",
        "      with open(f'{root_dir}/dna_corpus/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
        "          fp.write('\\n'.join(text_data))\n",
        "      text_data = []\n",
        "      file_count += 1\n",
        "  # after saving in 10K chunks, we will have ~2082 leftover samples, we save those now too\n",
        "  with open(f'{root_dir}/dna_corpus/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
        "    fp.write('\\n'.join(text_data))\n",
        "\n",
        "\n",
        "create_corpus(non_patho)\n",
        "create_corpus(patho)\n",
        "\n",
        "# f = open(\"/content/gdrive/My Drive/dna_seq_data/.txt\",'w')\n",
        "# for i in range(0,len(non_patho[1])):\n",
        "#   current_name = non_patho[0][i]\n",
        "#   current_seq = non_patho[1][i]\n",
        "#   tok = tokenized_sequence(current_seq,dyn_vocab)\n",
        "#   f.write(current_name + \";\" + \"0\" + \";\"  + str(tok) + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "48db4b9e6211487d93369fcd18a2b5d6",
            "6ad8b37bf3c94080ab389682904a1fe4",
            "cf1e8c18d7ad430aab3199a3843c0da8",
            "3e0e345f15a441498744edebaee0daf1",
            "f9074e279d5644dba800ad576b8d57f2",
            "a8fccfd9df4e48f584ccf4889e37f768",
            "bb022b8355a64c8982ba2ac1ad64d586",
            "f2a4b164348143e1a9b7f9405999eed4",
            "702b43def5f24adab9b9e3b3e8dd50da",
            "342afc0d69ac47748aeae044e0b20f6b",
            "c74fa26aa4c348d49646b2d9a3af1e07",
            "796be56d30fc45e7b92a888a9ed23616",
            "27cc742f50ed4c76a294b19bd84aa199",
            "bcef4b0ac0db442fa9da5b7a12b44660",
            "e3accf0bbd574238b6ad286804f75c56",
            "66da9b4d961f48cd8965b3450c51ed64",
            "2d0467d8fe074952916461dbf0cb9905",
            "f0afd2a523ec4d57840f898133512271",
            "d5976801351b45349ea8c356ee76fd0c",
            "d3e97a97379e4f1d86fc64e4fc064047",
            "67981d12c63a4dd2a7a2a13ff7fc8236",
            "14e01489975a4f998f843f43639ce2dc"
          ]
        },
        "id": "25IEtAt5kT1D",
        "outputId": "32268d2b-eb5e-45ee-9342-8a932b83c65c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/22707 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48db4b9e6211487d93369fcd18a2b5d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/18909 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "796be56d30fc45e7b92a888a9ed23616"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get paths of corpus\n",
        "from pathlib import Path\n",
        "paths = [str(x) for x in Path(f'{root_dir}/dna_corpus').glob('**/text*.txt')]\n",
        "paths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuAveOSCoQE6",
        "outputId": "46094d61-f75c-4551-e5e5-e91588162f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/gdrive/My Drive/sathya_dna_seq/dna_corpus/text_2.txt',\n",
              " '/content/gdrive/My Drive/sathya_dna_seq/dna_corpus/text_1.txt',\n",
              " '/content/gdrive/My Drive/sathya_dna_seq/dna_corpus/text_0.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the vocab in our dna corpus\n",
        "vocab = {}\n",
        "for path in paths:\n",
        "  fh = open(path,'r')\n",
        "  for line in fh:\n",
        "    word = line.strip()\n",
        "    vocab[word] = 1\n",
        "\n",
        "vocab_length = len(vocab)\n",
        "print(vocab_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWvYAqsxpDla",
        "outputId": "1766ce70-bb5d-422b-9704-7aea8e2078ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize corpus and save the vocab and tokenized corpus (Do not run)"
      ],
      "metadata": {
        "id": "YyERJ1CpwIwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Create tokens for above dna corpus\n",
        "\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "tokenizer.train(files=paths, vocab_size=vocab_length, min_frequency=2,\n",
        "                special_tokens=['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n",
        "tokenizer.save_model(f'{root_dir}/dna_corpus')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kj7d4-l7o9BS",
        "outputId": "3adeaf81-712e-4ac9-b42b-91b61a129020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/gdrive/My Drive/sathya_dna_seq//dna_corpus/vocab.json',\n",
              " '/content/gdrive/My Drive/sathya_dna_seq//dna_corpus/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize tokenizer with our corpus and vocab created using Tokenizer above\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "tokenizer = ByteLevelBPETokenizer.from_file(vocab_filename=f\"{root_dir}/dna_corpus/vocab.json\", \n",
        "                                  merges_filename=f\"{root_dir}/dna_corpus//merges.txt\")"
      ],
      "metadata": {
        "id": "KAmSyybUrnxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.encode('actg attg gttc nnnn')\n",
        "print(tokens.sequence_ids,tokens.ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mQhqR4rsnOe",
        "outputId": "1c8d3365-ded5-4587-9f4c-f9e04e421923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0] [857, 454, 568, 587]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Data pipeline"
      ],
      "metadata": {
        "id": "GGWgBm3xwSM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## initialize the tokenizer\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "tokenizer = ByteLevelBPETokenizer.from_file(vocab_filename=f\"{root_dir}/dna_corpus/vocab.json\", \n",
        "                                  merges_filename=f\"{root_dir}/dna_corpus//merges.txt\")\n",
        "\n",
        "tokenizer.enable_truncation(max_length=5200)\n",
        "tokenizer.enable_padding()"
      ],
      "metadata": {
        "id": "Kp6YVS-V2g1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(\"actg gtac gtat\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ly1wHYZgfxPB",
        "outputId": "9dd90372-6e91-435a-fc89-a713ed4c33f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=3, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine data from both and encode all our sample data.\n",
        "samples = pd.concat([patho, non_patho])\n",
        "# batch = tokenizer.encode_batch(samples[1])\n",
        "print(len(patho),len(non_patho))\n",
        "\n",
        "max_len = 0\n",
        "max_index = 0\n",
        "max_dna = \"\"\n",
        "for index,i in enumerate(samples[1]):\n",
        "  if max_len < len(i):\n",
        "    max_len = len(i)\n",
        "    max_index = index\n",
        "    max_dna = i\n",
        "\n",
        "seq_len = [len(i) for i in samples[1]]\n",
        "pd.Series(seq_len).hist(bins = 30)\n",
        "\n",
        "lens = [len(s) for s in samples[1]]\n",
        "avg = sum(lens) / len(lens)\n",
        "avg,min(lens),max(lens),max_len,max_index\n"
      ],
      "metadata": {
        "id": "LejSW3ZHxBvz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "c33d76bb-c0f4-43b4-91c9-6eff02d09358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18909 22707\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1409.815984236832, 499, 2986, 2986, 18864)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = tokenizer.encode_batch(samples[1])\n",
        "seq_len = [len(i) for i in batch]\n",
        "pd.Series(seq_len).hist(bins = 30)\n",
        "\n",
        "lens = [len(s) for s in batch]\n",
        "avg = sum(lens) / len(lens)\n",
        "avg,min(lens),max(lens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "pCH_wpldgpqt",
        "outputId": "7bcb928a-c5ce-4b7a-de3e-802306dc5096"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(598.0, 598, 598)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYaklEQVR4nO3df5Bd9Xnf8ffH4ocVbAw29pZImooM8mRkM8b2Fug4bTZ4AoJ0KjKxXQhjVJtaaQwde0oTQ9opiTEzdmNCCmPTykG1cFXLlNiV6sjGCubW4075aWOEwJQ1yEEKNhMkIGtPoLKf/nG/Sm6UXe1qd+9d7e77NXNnz33O95z7ffaO9Nlz7tk9qSokSYvbK+Z6ApKkuWcYSJIMA0mSYSBJwjCQJAHHzPUEpuuUU06plStXzvU0jsiPfvQjTjjhhLmexkDZ8+Jgz/PHgw8++BdV9fpD6/M2DFauXMkDDzww19M4Ip1Oh5GRkbmexkDZ8+Jgz/NHku+PV/c0kSTJMJAkGQaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSmMe/gSwdrXbufYF/fvWfTDpu98d/ZQCzkabGIwNJkmEgSTIMJEkYBpIkjiAMkixJ8u0kX27PT0tyb5LRJF9IclyrH9+ej7b1K3v2cU2rP57k/J76mlYbTXL1LPYnSZqCIzky+BDwWM/zTwA3VtXpwH7g8la/HNjf6je2cSRZDVwMvAlYA3y6BcwS4FPABcBq4JI2VpI0IFMKgyTLgV8B/qg9D3AucEcbsgm4qC2vbc9p69/Zxq8FtlTVS1X1FDAKnNUeo1X1ZFW9DGxpYyVJAzLV3zP4Q+C3gVe3568Dnq+qA+35HmBZW14GPA1QVQeSvNDGLwPu6dln7zZPH1I/e7xJJFkPrAcYGhqi0+lMcfpHh7GxsXk355lajD0PLYWrzjgw6biF9H1ZjO/zQut50jBI8k+AZ6vqwSQjfZ/RYVTVBmADwPDwcM23W87N19vkzcRi7PnmzVu5YefkP2ftvnSk/5MZkMX4Pi+0nqdyZPAO4J8muRB4JXAi8B+Bk5Ic044OlgN72/i9wApgT5JjgNcAz/XUD+rdZqK6JGkAJv3MoKquqarlVbWS7gfAX6+qS4G7gXe1YeuArW15W3tOW//1qqpWv7hdbXQasAq4D7gfWNWuTjquvca2WelOkjQlM/nbRB8BtiT5GPBt4NZWvxX4XJJRYB/d/9ypql1JbgceBQ4AV1TVTwCSXAncCSwBNlbVrhnMS5J0hI4oDKqqA3Ta8pN0rwQ6dMxfAe+eYPvrgevHqW8Hth/JXCRJs8ffQJYkGQaSJMNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJTCEMkrwyyX1JvpNkV5Lfa/XPJnkqyUPtcWarJ8lNSUaTPJzkbT37WpfkifZY11N/e5KdbZubkqQPvUqSJjCVm9u8BJxbVWNJjgW+meQrbd1vVdUdh4y/gO4tLVcBZwO3AGcneS1wLTAMFPBgkm1Vtb+N+QBwL92b3KwBvoIkaSCmcg/kqqqx9vTY9qjDbLIWuK1tdw9wUpJTgfOBHVW1rwXADmBNW3diVd3T7pV8G3DR9FuSJB2pKX1mkGRJkoeAZ+n+h35vW3V9OxV0Y5LjW20Z8HTP5nta7XD1PePUJUkDMqV7ILcb15+Z5CTgS0neDFwD/AA4DtgAfAT4aJ/mCUCS9cB6gKGhITqdTj9fbtaNjY3NuznP1GLseWgpXHXGgUnHLaTvy2J8nxdaz1MKg4Oq6vkkdwNrquqTrfxSkv8C/Jv2fC+womez5a22Fxg5pN5p9eXjjB/v9TfQDR6Gh4drZGRkvGFHrU6nw3yb80wtxp5v3ryVG3ZO/k9r96Uj/Z/MgCzG93mh9TyVq4le344ISLIU+GXgu+1cP+3Kn4uAR9om24DL2lVF5wAvVNUzwJ3AeUlOTnIycB5wZ1v3YpJz2r4uA7bOZpOSpMObypHBqcCmJEvohsftVfXlJF9P8nogwEPAv2zjtwMXAqPAj4H3AVTVviTXAfe3cR+tqn1t+YPAZ4GldK8i8koiSRqgScOgqh4G3jpO/dwJxhdwxQTrNgIbx6k/ALx5srlIkvrD30CWJBkGkiTDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiSmdtvLVya5L8l3kuxK8nutflqSe5OMJvlCkuNa/fj2fLStX9mzr2ta/fEk5/fU17TaaJKr+9CnJOkwpnJk8BJwblW9BTgTWNPubfwJ4MaqOh3YD1zexl8O7G/1G9s4kqwGLgbeBKwBPp1kSbud5qeAC4DVwCVtrCRpQCYNg+oaa0+PbY8CzgXuaPVNwEVteW17Tlv/znaj+7XAlqp6qaqeonuP5LPaY7Sqnqyql4EtbawkaUCm9JlB+wn+IeBZYAfwPeD5qjrQhuwBlrXlZcDTAG39C8DreuuHbDNRXZI0IMdMZVBV/QQ4M8lJwJeAn+/npCaSZD2wHmBoaIhOpzMX05i2sbGxeTfnmVqMPQ8thavOODDpuIX0fVmM7/NC63lKYXBQVT2f5G7gHwInJTmm/fS/HNjbhu0FVgB7khwDvAZ4rqd+UO82E9UPff0NwAaA4eHhGhkZOZLpz7lOp8N8m/NMLcaeb968lRt2Tv5Pa/elI/2fzIAsxvd5ofU8lauJXt+OCEiyFPhl4DHgbuBdbdg6YGtb3tae09Z/vaqq1S9uVxudBqwC7gPuB1a1q5OOo/sh87ZZ6E2SNEVTOTI4FdjUrvp5BXB7VX05yaPAliQfA74N3NrG3wp8LskosI/uf+5U1a4ktwOPAgeAK9rpJ5JcCdwJLAE2VtWuWetQkjSpScOgqh4G3jpO/Um6VwIdWv8r4N0T7Ot64Ppx6tuB7VOYrySpD/wNZEmSYSBJMgwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJYmq3vVyR5O4kjybZleRDrf67SfYmeag9LuzZ5poko0keT3J+T31Nq40mubqnflqSe1v9C+32l5KkAZnKkcEB4KqqWg2cA1yRZHVbd2NVndke2wHauouBNwFrgE8nWdJum/kp4AJgNXBJz34+0fZ1OrAfuHyW+pMkTcGkYVBVz1TVt9ryXwKPAcsOs8laYEtVvVRVTwGjdG+PeRYwWlVPVtXLwBZgbZIA5wJ3tO03ARdNsx9J0jRMeg/kXklW0r0f8r3AO4Ark1wGPED36GE/3aC4p2ezPfxNeDx9SP1s4HXA81V1YJzxh77+emA9wNDQEJ1O50imP+fGxsbm3ZxnajH2PLQUrjrjwKTjFtL3ZTG+zwut5ymHQZJXAX8MfLiqXkxyC3AdUO3rDcD7+zLLpqo2ABsAhoeHa2RkpJ8vN+s6nQ7zbc4ztRh7vnnzVm7YOfk/rd2XjvR/MgOyGN/nhdbzlMIgybF0g2BzVX0RoKp+2LP+M8CX29O9wIqezZe3GhPUnwNOSnJMOzroHS9JGoCpXE0U4Fbgsar6g576qT3DfhV4pC1vAy5OcnyS04BVwH3A/cCqduXQcXQ/ZN5WVQXcDbyrbb8O2DqztiRJR2IqRwbvAN4L7EzyUKv9Dt2rgc6ke5poN/AbAFW1K8ntwKN0r0S6oqp+ApDkSuBOYAmwsap2tf19BNiS5GPAt+mGjyRpQCYNg6r6JpBxVm0/zDbXA9ePU98+3nZV9STdq40kSXPA30CWJBkGkiTDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiSmdtvLFUnuTvJokl1JPtTqr02yI8kT7evJrZ4kNyUZTfJwkrf17GtdG/9EknU99bcn2dm2uandalOSNCBTOTI4AFxVVauBc4ArkqwGrgbuqqpVwF3tOcAFdO97vApYD9wC3fAArgXOpntXs2sPBkgb84Ge7dbMvDVJ0lRNGgZV9UxVfast/yXwGLAMWAtsasM2ARe15bXAbdV1D3BSklOB84EdVbWvqvYDO4A1bd2JVXVPVRVwW8++JEkDMOk9kHslWQm8FbgXGKqqZ9qqHwBDbXkZ8HTPZnta7XD1PePUx3v99XSPNhgaGqLT6RzJ9Ofc2NjYvJvzTC3GnoeWwlVnHJh03EL6vizG93mh9TzlMEjyKuCPgQ9X1Yu9p/WrqpJUH+b3t1TVBmADwPDwcI2MjPT7JWdVp9Nhvs15phZjzzdv3soNOyf/p7X70pH+T2ZAFuP7vNB6ntLVREmOpRsEm6vqi638w3aKh/b12VbfC6zo2Xx5qx2uvnycuiRpQKZyNVGAW4HHquoPelZtAw5eEbQO2NpTv6xdVXQO8EI7nXQncF6Sk9sHx+cBd7Z1LyY5p73WZT37kiQNwFROE70DeC+wM8lDrfY7wMeB25NcDnwfeE9btx24EBgFfgy8D6Cq9iW5Dri/jftoVe1ryx8EPgssBb7SHpKkAZk0DKrqm8BE1/2/c5zxBVwxwb42AhvHqT8AvHmyuUiS+sPfQJYkGQaSJMNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJKZ228uNSZ5N8khP7XeT7E3yUHtc2LPumiSjSR5Pcn5PfU2rjSa5uqd+WpJ7W/0LSY6bzQYlSZObypHBZ4E149RvrKoz22M7QJLVwMXAm9o2n06yJMkS4FPABcBq4JI2FuATbV+nA/uBy2fSkCTpyE0aBlX1DWDfZOOatcCWqnqpqp6iex/ks9pjtKqerKqXgS3A2iQBzgXuaNtvAi46shYkSTM16T2QD+PKJJcBDwBXVdV+YBlwT8+YPa0G8PQh9bOB1wHPV9WBccb/HUnWA+sBhoaG6HQ6M5j+4I2Njc27Oc/UYux5aClcdcaBScctpO/LYnyfF1rP0w2DW4DrgGpfbwDeP1uTmkhVbQA2AAwPD9fIyEi/X3JWdTod5tucZ2ox9nzz5q3csHPyf1q7Lx3p/2QGZDG+zwut52mFQVX98OByks8AX25P9wIreoYubzUmqD8HnJTkmHZ00DtekjQg07q0NMmpPU9/FTh4pdE24OIkxyc5DVgF3AfcD6xqVw4dR/dD5m1VVcDdwLva9uuArdOZkyRp+iY9MkjyeWAEOCXJHuBaYCTJmXRPE+0GfgOgqnYluR14FDgAXFFVP2n7uRK4E1gCbKyqXe0lPgJsSfIx4NvArbPVnCRpaiYNg6q6ZJzyhP9hV9X1wPXj1LcD28epP0n3aiNJ0hzxN5AlSYaBJMkwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJIkphEGSjUmeTfJIT+21SXYkeaJ9PbnVk+SmJKNJHk7ytp5t1rXxTyRZ11N/e5KdbZubkmS2m5QkHd5Ujgw+C6w5pHY1cFdVrQLuas8BLqB73+NVwHrgFuiGB93bZZ5N965m1x4MkDbmAz3bHfpakqQ+mzQMquobwL5DymuBTW15E3BRT/226roHOCnJqcD5wI6q2ldV+4EdwJq27sSquqeqCritZ1+SpAGZ9B7IExiqqmfa8g+Aoba8DHi6Z9yeVjtcfc849XElWU/3iIOhoSE6nc40pz83xsbG5t2cZ2ox9jy0FK4648Ck4xbS92Uxvs8LrefphsFfq6pKUrMxmSm81gZgA8Dw8HCNjIwM4mVnTafTYb7NeaYWY883b97KDTsn/6e1+9KR/k9mQBbj+7zQep7u1UQ/bKd4aF+fbfW9wIqecctb7XD15ePUJUkDNN0w2AYcvCJoHbC1p35Zu6roHOCFdjrpTuC8JCe3D47PA+5s615Mck67iuiynn1JkgZk0mPZJJ8HRoBTkuyhe1XQx4Hbk1wOfB94Txu+HbgQGAV+DLwPoKr2JbkOuL+N+2hVHfxQ+oN0r1haCnylPSRJAzRpGFTVJROseuc4Ywu4YoL9bAQ2jlN/AHjzZPOQJPWPv4EsSTIMJEmGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEnMMAyS7E6yM8lDSR5otdcm2ZHkifb15FZPkpuSjCZ5OMnbevazro1/Ism6iV5PktQfs3Fk8EtVdWZVDbfnVwN3VdUq4K72HOACYFV7rAdugW540L2V5tnAWcC1BwNEkjQY/ThNtBbY1JY3ARf11G+rrnuAk5KcCpwP7KiqfVW1H9gBrOnDvCRJE5j0HsiTKOBrSQr4z1W1ARiqqmfa+h8AQ215GfB0z7Z7Wm2i+t+RZD3dowqGhobodDoznP5gjY2Nzbs5z9Ri7HloKVx1xoFJxy2k78tifJ8XWs8zDYNfqKq9Sd4A7Ejy3d6VVVUtKGZFC5sNAMPDwzUyMjJbux6ITqfDfJvzTC3Gnm/evJUbdk7+T2v3pSP9n8yALMb3eaH1PKPTRFW1t319FvgS3XP+P2ynf2hfn23D9wIrejZf3moT1SVJAzLtMEhyQpJXH1wGzgMeAbYBB68IWgdsbcvbgMvaVUXnAC+000l3AuclObl9cHxeq0mSBmQmp4mGgC8lObif/1ZVX01yP3B7ksuB7wPvaeO3AxcCo8CPgfcBVNW+JNcB97dxH62qfTOYlyTpCE07DKrqSeAt49SfA945Tr2AKybY10Zg43TnIkmaGX8DWZJkGEiSDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSOIrCIMmaJI8nGU1y9VzPR5IWk6MiDJIsAT4FXACsBi5JsnpuZyVJi8dREQbAWcBoVT1ZVS8DW4C1czwnSVo0pn0P5Fm2DHi65/ke4OxDByVZD6xvT8eSPD6Auc2mU4C/mOtJDJg9TyCfGMBMBsf3ef74++MVj5YwmJKq2gBsmOt5TFeSB6pqeK7nMUj2vDjY8/x3tJwm2gus6Hm+vNUkSQNwtITB/cCqJKclOQ64GNg2x3OSpEXjqDhNVFUHklwJ3AksATZW1a45nlY/zNtTXDNgz4uDPc9zqaq5noMkaY4dLaeJJElzyDCQJBkGM5Fkd5KdSR5K8kCrvSXJ/2n1/5nkxFa/tI07+PhpkjMn2O+/SvLdJLuS/IcBtjSpfvSc5Mwk9xzcZ5KzBtzWYR1hz8cm2dTqjyW5ZoJ9npbk3vbnV77QLpw4KvSp383tz808kmRjkmMH2dNk+tFzz75vSjI2iD5mpKp8TPMB7AZOOaR2P/CLbfn9wHXjbHcG8L0J9vlLwJ8Cx7fnb5jrPgfQ89eAC9ryhUBnrvucbs/ArwNb2vLPtG1XjrPP24GL2/J/An5zrvvsc78XAmmPzx9N/far57Z+GPgcMDbXPU728Mhg9r0R+EZb3gH82jhjLqH7JzfG85vAx6vqJYCqenbWZzj7ZtpzASe25dcAfz6rs+uPiXou4IQkxwBLgZeBF3s3TBLgXOCOVtoEXNTn+c7UtPsFqKrt1QD30f1doqPdjHpuf3Pt94Hf7v9UZ84wmJkCvpbkwfanMgB28Td/V+nd/O1fpjvon9H96Wg8bwT+UTuF8L+S/INZnfHM9aPnDwO/n+Rp4JPAYQ+758CR9HwH8CPgGeDPgE9W1b5D9vc64PmqOtCe76H7J1mOFrPd719rp4feC3y1HxOfgX70fCWwraqe6d+0Z9FcH5rM5wewrH19A/Ad4B8DP0/3tMeDwLXAc4dsczaw8zD7fAS4me7h9FnAU7RLgI+GR596vgn4tbb8HuBP57rP6fYMvAPYDBzbxj8O/Nwh+zuF7h9mPPh8BfDIXPfZr34P2fdngD+c6x4H8B7/LPBN4Jj23NNEC1lV7W1fnwW+BJxVVd+tqvOq6u10fxL+3iGbXczEPyFD96fEL1bXfcBP6f7ncVToU8/rgC+25f9ONwSPGkfY868DX62q/9fG/2+65417PQec1E4zwFH251f60C8ASa4FXg/86373cKT60PNbgdOB0SS7gZ9JMjqAVqbNMJimJCckefXBZeA84JEkb2i1VwD/ju6Hg/TU3sPE584B/gfdD5FJ8kbgOI6Sv4zYx57/HPjFtnwu8MTsz356ptHzn9Ht4eD4c4Dv9u6zuj8q3g28q5XWAVv728nU9KPftu5fAOcDl1TVT/vdx5Ho03v8J1X196pqZVWtBH5cVacPop9pm+tDk/n6AH6O7uHkd+ieW/y3rf4h4P+2x8fpOcUDjAD3jLOvPwKG2/JxwH+le7roW8C5c93rAHr+BbqH4t8B7gXePte9Trdn4FV0j252AY8Cv9Wzr+3Az/bs9z5gtI0/fq577XO/B+j+ZP1Qe/z7ue613z0f8hpH/Wki/xyFJMnTRJIkw0CShGEgScIwkCRhGEiSMAwkSRgGkiTg/wMPj4fRjdFNUQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert(batch[18864].tokens == tokenizer.encode(max_dna).tokens)\n",
        "len(max_dna.split(' '))\n",
        "# tokenizer.encode(max_dna).tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xc5rbyPTh7JW",
        "outputId": "606fb55a-2cb2-485f-82af-fe6054b23990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "598"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert our batch into tensors \n",
        "import torch\n",
        "labels = torch.tensor([x.ids for x in batch])\n",
        "mask = torch.tensor([x.attention_mask for x in batch])"
      ],
      "metadata": {
        "id": "2M_pg87VxVx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make copy of labels tensor, this will be input_ids\n",
        "input_ids = labels.detach().clone()\n",
        "# create random array of floats with equal dims to input_ids\n",
        "rand = torch.rand(input_ids.shape)\n",
        "# mask random 15% where token is not 0 [PAD], 1 [CLS], or 2 [SEP]\n",
        "mask_arr = (rand < .15) * (input_ids != 0) * (input_ids != 1) * (input_ids != 2)\n",
        "# loop through each row in input_ids tensor (cannot do in parallel)\n",
        "for i in range(input_ids.shape[0]):\n",
        "    # get indices of mask positions from mask array\n",
        "    selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
        "    # mask input_ids\n",
        "    input_ids[i, selection] = 3  # our custom [MASK] token == 3\n"
      ],
      "metadata": {
        "id": "j-qqnY7y0FT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tG51poG0Ho2",
        "outputId": "406d2909-9777-46c3-d49e-fdf237b5904d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([41616, 598])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build DataLoader"
      ],
      "metadata": {
        "id": "gGhX01Tb3Oew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encodings = {'input_ids': input_ids, 'attention_mask': mask, 'labels': labels}\n"
      ],
      "metadata": {
        "id": "f2TaMBbO0YBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        # store encodings internally\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __len__(self):\n",
        "        # return the number of samples\n",
        "        return self.encodings['input_ids'].shape[0]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # return dictionary of input_ids, attention_mask, and labels for index i\n",
        "        return {key: tensor[i] for key, tensor in self.encodings.items()}"
      ],
      "metadata": {
        "id": "4He_hZ4D3nJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Dataset(encodings)"
      ],
      "metadata": {
        "id": "9zjv0OD73vdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "oUN_9WZ73xzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Train BERT Model\n"
      ],
      "metadata": {
        "id": "06MU5lc7374H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the model with the config\n",
        "from transformers import RobertaConfig\n",
        "\n",
        "print(tokenizer.get_vocab_size())\n",
        "# assert 21615 == tokenizer.get_vocab_size()\n",
        "\n",
        "config = RobertaConfig(\n",
        "    vocab_size=tokenizer.get_vocab_size(),  # we align this to the tokenizer vocab_size\n",
        "    max_position_embeddings=1200,\n",
        "    hidden_size=768,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1\n",
        ")"
      ],
      "metadata": {
        "id": "JxNHvpbK4Cbo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "152c4bde-f713-486e-b671-2e9fa5ae92b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaForMaskedLM\n",
        "from transformers import AdamW\n",
        "\n",
        "model = RobertaForMaskedLM(config)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# and move our model over to the selected device\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# activate training mode\n",
        "model.train()\n",
        "# initialize optimizer\n",
        "optim = AdamW(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fV9Cf8Z548Xu",
        "outputId": "aeb41a29-f0e4-4b0b-e5ee-ffdc1bea59d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "a2jEGaOjrHda",
        "outputId": "c7a3a25e-c92d-4f23-8452-2d73978ccc7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForMaskedLM(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(2163, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(1200, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): RobertaLMHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (decoder): Linear(in_features=768, out_features=2163, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training "
      ],
      "metadata": {
        "id": "HJXsCZI86L-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter(f\"{root_dir}/logging_dir\")\n"
      ],
      "metadata": {
        "id": "Nlusndo-iVwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "epochs = 2\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # setup loop with TQDM and dataloader\n",
        "    loop = tqdm(loader, leave=True)\n",
        "    for bi,batch in enumerate(loop):\n",
        "        # initialize calculated gradients (from prev step)\n",
        "        optim.zero_grad()\n",
        "        # pull all tensor batches required for training\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        # process\n",
        "        outputs = model(input_ids, attention_mask=attention_mask,\n",
        "                        labels=labels)\n",
        "        # extract loss\n",
        "        loss = outputs.loss\n",
        "        # calculate loss for every parameter that needs grad update\n",
        "        loss.backward()\n",
        "        # update parameters\n",
        "        optim.step()\n",
        "        # print relevant info to progress bar\n",
        "        writer.add_scalar(\"Loss/train\", loss, bi)\n",
        "        loop.set_description(f'Epoch {epoch}')\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "    writer.flush()\n",
        "writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "1225b02cb11e4201b5fd2999aeb29775",
            "4e58fbe6e0fd4a638a0da84899912804",
            "b89faa3d21594ff1ad8dae70940827a9",
            "48c1ad030a0e4dfda1f1a09024a2dae2",
            "bf00842609e34aeaa55f8473afd42ab7",
            "c253d729f94f4c5d89b57b648fb99f32",
            "39fd83db939141c4b79949d9b1abd9c9",
            "c7b9c85f667f4363a4a115be4716f9c7",
            "9764d39196c8485b970211bffd8453bc",
            "2d099a92b61e4569ac8b53b804f03338",
            "78e6744365b349d8a1ca3937bcec595b",
            "cc079af96e3b45108cc563816c66cba1",
            "7f5e25b6dcc64c56875821be575d2616",
            "e2baaae531fb43548a6dbc03a047247d",
            "72af49f2b24f4dea8b944b52b4924a92",
            "371bb1f82fc145cda04f19e1501bf157",
            "f9f8b6f25c9d4656a6e5d33e5cf98cd0",
            "a0ea341535ef423a8acdf840851f6b84",
            "84c2442348d34039be134cf4c1026e15",
            "825fc53f71924cf49347110f5c9ce7f1",
            "0f8d458c5c4543a690dc7809ed758e92",
            "cc394a7ede2543e1aa6cabc71ff55a59"
          ]
        },
        "id": "XtAK-16r59e5",
        "outputId": "886e625a-2a57-4206-85ea-d7a40eb892a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1225b02cb11e4201b5fd2999aeb29775",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2601 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc079af96e3b45108cc563816c66cba1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2601 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir f'{root_dir}/logging_dir'"
      ],
      "metadata": {
        "id": "HsoxsPu-i5NC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "outputId": "da263fad-345b-4709-9f2f-131596708ec1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(f'{root_dir}/dnaBerTo_1200')  # and don't forget to save filiBERTo!"
      ],
      "metadata": {
        "id": "bMz4sj-F7fPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing trained model"
      ],
      "metadata": {
        "id": "wzLVNAsMKw8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(samples[1][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByiZx5fvuwoB",
        "outputId": "2e4f8f53-94a8-4e9e-8592-0de1dca6cf02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1    cgtg ctta acac atgc aagt cgaa cggg tctg cctt g...\n",
            "1    acga acgc tggc ggcg tgct taac acat gcaa gtcg a...\n",
            "Name: 1, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaForMaskedLM\n",
        "\n",
        "model = RobertaForMaskedLM.from_pretrained(f\"{root_dir}/dnaBerTo_1200\")\n",
        "## initialize the tokenizer\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "tokenizer = ByteLevelBPETokenizer.from_file(vocab_filename=f\"{root_dir}/dna_corpus/vocab.json\", \n",
        "                                  merges_filename=f\"{root_dir}/dna_corpus//merges.txt\")\n",
        "\n",
        "tokenizer.enable_truncation(max_length=1200)\n",
        "tokenizer.enable_padding()\n",
        "\n",
        "samples = [\"atgc aagt cgaa cggg tctg cctt gttt tttg cggg gtgg gtta gtgg cgaa cggg tgag taac acgt gagt aacc tgcc \",\"actt attg\"]\n",
        "# expected aagg cagg tctc tggg cagt aact gacg ctga ggag cgaa agca tggg tagc gaac agga ttag atac cctg gtag tcca tgcc gtaa\n",
        "import torch\n",
        "batch = tokenizer.encode_batch(samples)\n",
        "labels = torch.tensor([x.ids for x in batch])\n",
        "mask = torch.tensor([x.attention_mask for x in batch])\n",
        "# make copy of labels tensor, this will be input_ids\n",
        "input_ids = labels.detach().clone()\n",
        "# create random array of floats with equal dims to input_ids\n",
        "rand = torch.rand(input_ids.shape)\n",
        "# mask random 15% where token is not 0 [PAD], 1 [CLS], or 2 [SEP]\n",
        "mask_arr = (rand < .15) * (input_ids != 0) * (input_ids != 1) * (input_ids != 2)\n",
        "# loop through each row in input_ids tensor (cannot do in parallel)\n",
        "for i in range(input_ids.shape[0]):\n",
        "    # get indices of mask positions from mask array\n",
        "    selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
        "    # mask input_ids\n",
        "    input_ids[i, selection] = 3  # our custom [MASK] token == 3\n",
        "input_ids"
      ],
      "metadata": {
        "id": "rmHph1G_O4Td",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80ceaa56-0e75-42a4-936b-00a375c02a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[614, 362, 414, 344, 462, 358,   3, 539, 344, 328, 434, 328, 414, 344,\n",
              "         361, 366, 456, 417, 425,   3, 225],\n",
              "        [662, 454,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0]])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "model.to(device)\n",
        "with torch.no_grad():\n",
        "  prediction = model(input_ids.to(device), attention_mask=mask.to(device))\n",
        "                        # labels=labels.to(device))\n",
        "\n",
        "print(prediction['logits'].shape)\n",
        "prediction = prediction['logits'][0].cpu()\n",
        "print(prediction.shape)\n",
        "pred_tok = torch.argmax(prediction, dim=-1)\n",
        "print(pred_tok.shape)\n",
        "print(pred_tok)\n",
        "tok = pred_tok.numpy()\n",
        "print(f\" tok: {tok}\")\n",
        "predicted_text = tokenizer.decode(tok)\n",
        "print(samples[0])\n",
        "print(predicted_text)\n",
        "# prediction = prediction['logits']\n",
        "# output = torch.argmax(prediction[0]).item()\n",
        "# tokenizer.decode()\n",
        "# prediction = prediction[0][0].argmax(dim=-1).item()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isJMWdVUTN0o",
        "outputId": "4376ba51-0416-4dc7-858e-5bad81a08da2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 21, 2163])\n",
            "torch.Size([21, 2163])\n",
            "torch.Size([21])\n",
            "tensor([614, 362, 414, 344, 462, 358, 477, 539, 344, 328, 434, 328, 414, 344,\n",
            "        361, 366, 456, 417, 425, 320, 225])\n",
            " tok: [614 362 414 344 462 358 477 539 344 328 434 328 414 344 361 366 456 417\n",
            " 425 320 225]\n",
            "atgc aagt cgaa cggg tctg cctt gttt tttg cggg gtgg gtta gtgg cgaa cggg tgag taac acgt gagt aacc tgcc \n",
            "atgc aagt cgaa cggg tctg cctt tgat tttg cggg gtgg gtta gtgg cgaa cggg tgag taac acgt gagt aacc ggga \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patho[1][0][0:100],patho[1][0][100:200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlIo7qjvfM8n",
        "outputId": "ed16c450-7cce-47fc-9db2-b3a2c35412f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('atgc aagt cgaa cggg tctg cctt gttt tttg cggg gtgg gtta gtgg cgaa cggg tgag taac acgt gagt aacc tgcc ',\n",
              " 'cctc actt ctgg ataa ccgc ttga aagg gtgg ctaa tacg gggt gttc tggc tgtg ccgc atgg tgtg gctg ggaa agat ')"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tune dnaBerTo model for patho/non-patho classification (Do not run , Moved to new notebook)"
      ],
      "metadata": {
        "id": "rKdMWyKU-iU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Refer to https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/"
      ],
      "metadata": {
        "id": "hqoGDO9A-ofG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "LhDR0Tzc-tOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "tokenizer = ByteLevelBPETokenizer.from_file(vocab_filename=f\"{root_dir}/dna_corpus/vocab.json\", \n",
        "                                  merges_filename=f\"{root_dir}/dna_corpus//merges.txt\")\n",
        "tokenizer.enable_truncation(max_length=512)\n",
        "tokenizer.enable_padding()\n",
        "\n",
        "import pandas as pd\n",
        "data = f\"/{root_dir}/cleaned_non_patho_data_word.txt\"\n",
        "non_patho = pd.read_csv(data, sep=';', header=None)\n",
        "\n",
        "data = f\"/{root_dir}/cleaned_patho_data_word.txt\"\n",
        "patho = pd.read_csv(data, sep=';', header=None)\n",
        "\n",
        "# split train dataset into train, validation and test sets\n",
        "# Combine data from both and encode all our sample data.\n",
        "patho['label'] = 1\n",
        "non_patho['label'] = 0\n",
        "samples = pd.concat([patho, non_patho])\n",
        "# batch = tokenizer.encode_batch(samples[1])\n",
        "\n",
        "# print(len(patho),len(non_patho))\n",
        "\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(samples[1], samples['label'], \n",
        "                                                                    random_state=2018, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=samples['label'])\n",
        "\n",
        "\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=2018, \n",
        "                                                                test_size=0.5, \n",
        "                                                                stratify=temp_labels)\n"
      ],
      "metadata": {
        "id": "rcGhx5EFn5E2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample data\n",
        "text = \"atgc aagt cgaa cggg tctg cctt gttt tttg cggg gtgg gtta gtgg cgaa cggg tgag taac acgt gagt aacc tgcc\"\n",
        "\n",
        "# encode text\n",
        "sent_id = tokenizer.encode(text)\n",
        "\n",
        "# output\n",
        "print(sent_id.ids)\n",
        "tokenizer.decode(sent_id.ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "mzw6UE7mo3UH",
        "outputId": "235b7596-b858-4326-b0a4-d5e488480594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[614, 362, 414, 344, 462, 358, 552, 539, 344, 328, 434, 328, 414, 344, 361, 366, 456, 417, 425, 376]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'atgc aagt cgaa cggg tctg cctt gttt tttg cggg gtgg gtta gtgg cgaa cggg tgag taac acgt gagt aacc tgcc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = [len(i.split()) for i in train_text]\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "5BPgzX26o4FL",
        "outputId": "802bfb90-598a-4e10-9b6c-874234b685e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.encode_batch(\n",
        "    train_text.tolist(),\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.encode_batch(\n",
        "    val_text.tolist(),\n",
        "    \n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.encode_batch(\n",
        "    test_text.tolist(),\n",
        ")"
      ],
      "metadata": {
        "id": "qciIbkuLrBE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## convert lists to tensors\n",
        "\n",
        "train_seq = torch.tensor([x.ids for x in tokens_train])\n",
        "train_mask = torch.tensor([x.attention_mask for x in tokens_train])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "val_seq = torch.tensor([x.ids for x in tokens_val])\n",
        "val_mask = torch.tensor([x.attention_mask for x in tokens_val])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "test_seq = torch.tensor([x.ids for x in tokens_test])\n",
        "test_mask = torch.tensor([x.attention_mask for x in tokens_test])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "metadata": {
        "id": "IUZ5D4QOriI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size\n",
        "batch_size = 32\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "-mMlTmekr81M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class DnaPathoBERT(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      \n",
        "      super(DnaPathoBERT, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      \n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      \n",
        "      # dense layer 2 (Output layer)\n",
        "      self.fc2 = nn.Linear(512,2)\n",
        "\n",
        "      #softmax activation function\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      # preds = self.bert(sent_id, attention_mask=mask,labels=sent_id)\n",
        "      # preds = torch.argmax(preds['logits'], axis = -1).float()\n",
        "      # print(f'bert shape: {preds.shape}')\n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
        "      x = self.fc1(cls_hs)\n",
        "      x = self.fc1(preds)\n",
        "\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # output layer\n",
        "      x = self.fc2(x)\n",
        "      \n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "kcoEvcistmz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaForMaskedLM\n",
        "from transformers import AdamW\n",
        "\n",
        "from transformers import RobertaConfig\n",
        "vocab_length = 21615\n",
        "config = RobertaConfig(\n",
        "    vocab_size=vocab_length,  # we align this to the tokenizer vocab_size\n",
        "    max_position_embeddings=514,\n",
        "    hidden_size=768,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1\n",
        ")\n",
        "\n",
        "# pass the pre-trained BERT to our define architecture\n",
        "dna_base_model = RobertaForMaskedLM.from_pretrained(f\"{root_dir}/dnaBerTo\",config=config)\n",
        "\n",
        "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# and move our model over to the selected device\n",
        "# dna_base_model.to(device)\n",
        "\n",
        "# dna_base_model.to(device)"
      ],
      "metadata": {
        "id": "YOptxjUmtsLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dna_base_model.config.output_hidden_states=True\n",
        "\n",
        "# from transformers import RobertaConfig\n",
        "# model_name = 'roberta-base'\n",
        "# new_model = RobertaForMaskedLM.from_pretrained(model_name, config=dna_base_model.config,ignore_mismatched_sizes=True)\n",
        "# new_model.roberta = dna_base_model.roberta # Copy the weights from the original model\n",
        "# new_model.num_labels = 0\n",
        "# new_model.classifier = None\n",
        "\n",
        "# dna_base_model.config.num_attention_heads\n",
        "# layer_to_prune = dna_base_model.config.num_hidden_layers - 1\n",
        "# heads_to_prune = list(range(dna_base_model.config.num_attention_heads))\n",
        "\n",
        "# dna_base_model.roberta.prune_heads({layer_to_prune: heads_to_prune})\n",
        "\n",
        "# dna_base_model.config\n",
        "# new_model = RobertaForMaskedLM.from_pretrained(f\"{root_dir}/dnaBerTo\", config=dna_base_model.config,ignore_mismatched_sizes=True)\n",
        "# new_model.roberta = dna_base_model.roberta # Copy the weights from the original model\n",
        "# new_model\n",
        "# dna_base_model\n",
        "\n",
        "# dna_base_model_1 = torch.nn.Sequential(*(list(dna_base_model.children())[:-1])) \n",
        "# new_model\n",
        "\n",
        "# layer_to_prune = dna_base_model.config.num_hidden_layers - 1\n",
        "# heads_to_prune = list(range(dna_base_model.config.num_attention_heads))\n",
        "# dna_base_model.roberta.prune_heads({layer_to_prune: heads_to_prune})\n",
        "\n",
        "# dna_base_model.lm_head = torch.nn.Sequential(*(list(dna_base_model.lm_head.children())[:-1])) \n",
        "# dna_base_model.config.num_hidden_layers = dna_base_model.config.num_hidden_layers-1\n",
        "# new_model = RobertaForMaskedLM.from_pretrained(model_name, config=dna_base_model.config,ignore_mismatched_sizes=True)\n",
        "# new_model.roberta = dna_base_model.roberta\n",
        "# dna_base_model"
      ],
      "metadata": {
        "id": "jKrPQtLvFmCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DnaPathoBERT(dna_base_model)\n",
        "# push the model to GPU\n",
        "# model = model.to(device)\n",
        "\n",
        "# freeze all the parameters\n",
        "for param in model.bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "dyKuQSbjt9Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Test sample\n",
        "text = \"atgc aagt cgaa cggg tctg cctt gttt tttg cggg gtgg gtta gtgg cgaa cggg tgag taac acgt gagt aacc tgcc\"\\\n",
        "       \"<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> \"\\\n",
        "       \"<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> \"\\\n",
        "       \"<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> \"\\\n",
        "       \"<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> \"\\\n",
        "       \"<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> \"\\\n",
        "       \"<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> \"\n",
        "sent_id = tokenizer.encode(text)\n",
        "ids = torch.tensor([sent_id.ids])\n",
        "mask = torch.tensor([sent_id.attention_mask])\n",
        "print(len(sent_id.ids))\n",
        "dna_base_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  preds = dna_base_model(ids,mask,ids)\n",
        "  # preds = dna_base_model(ids.to(device),mask.to(device),ids.to(device))\n",
        "\n",
        "preds\n",
        "# preds = torch.argmax(preds['logits'], axis = -1).float()\n",
        "# model.fc1(preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "pXjdBnk66KBv",
        "outputId": "aed8146e-7d29-4859-80eb-23513a942a45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-ab2e8ed02981>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdna_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0;31m# preds = dna_base_model(ids.to(device),mask.to(device),ids.to(device))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m         outputs = self.roberta(\n\u001b[0m\u001b[1;32m   1098\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m    845\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2208\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 1e-4)          # learning rate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2B8NwXBnuZxV",
        "outputId": "a867afb4-414a-45bc-d110-ddcbdf83a022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=[0,1], y=train_labels)\n",
        "\n",
        "print(\"Class Weights:\",class_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bxPpCThu1pV",
        "outputId": "aac4585e-080d-4265-cc3b-12e9d98134c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Weights: [0.91635735 1.10044575]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #converting list of class weights to a tensor\n",
        "weights= torch.tensor(class_weights,dtype=torch.float)\n",
        "\n",
        "# push to GPU\n",
        "weights = weights.to(device)\n",
        "\n",
        "# define the loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "FJWx_6D3u4KK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Test the model \n",
        "for step,batch in enumerate(train_dataloader):\n",
        "  # push the batch to gpu\n",
        "  batch = [r.to(device) for r in batch]\n",
        "  sent_id, mask, labels = batch\n",
        "  _, cls_hs = model.bert(sent_id, attention_mask=mask,labels=sent_id)      \n",
        "  print(cls_hs)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CV5nZoELwt5w",
        "outputId": "872b5824-b13d-4d9d-c912-81459895baa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-434-a023931d8632>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0msent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_hs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msent_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_hs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m         outputs = self.roberta(\n\u001b[0m\u001b[1;32m   1098\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    849\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m         )\n\u001b[0;32m--> 851\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    524\u001b[0m                 )\n\u001b[1;32m    525\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    527\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    412\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 338\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mmixed_query_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# If this is instantiated as a cross-attention module, the keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasLtMatmul( ltHandle, computeDesc.descriptor(), &alpha_val, mat1_ptr, Adesc.descriptor(), mat2_ptr, Bdesc.descriptor(), &beta_val, result_ptr, Cdesc.descriptor(), result_ptr, Cdesc.descriptor(), &heuristicResult.algo, workspace.data_ptr(), workspaceSize, at::cuda::getCurrentCUDAStream())`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        " \n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "d31sD5vjvWEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      \n",
        "      # Calculate elapsed time in minutes.\n",
        "      elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "TtJJG6lTwHRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = val_dataloader.dataset[0:1]\n",
        "batch = [t.to(device) for t in batch]\n",
        "sent_id, mask, labels = batch\n",
        "with torch.no_grad():\n",
        "  print(sent_id.shape)\n",
        "  preds = model(sent_id, mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "iUBmxPy1BT2r",
        "outputId": "d6349bb2-2d33-4689-f455-6f1d8ecf8696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 478])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-283-4b5ecce04c28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-264-3d2c734195e3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sent_id, mask)\u001b[0m\n\u001b[1;32m     31\u001b[0m       \u001b[0;31m# print(f'bert shape: {preds.shape}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0;31m# x = self.fc1(cls_hs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x478 and 512x512)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "19V8C-6nwLwe",
        "outputId": "2e050481-8f96-42dc-d678-60c2dd18d8b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 10\n",
            "  Batch    50  of    911.\n",
            "  Batch   100  of    911.\n",
            "  Batch   150  of    911.\n",
            "  Batch   200  of    911.\n",
            "  Batch   250  of    911.\n",
            "  Batch   300  of    911.\n",
            "  Batch   350  of    911.\n",
            "  Batch   400  of    911.\n",
            "  Batch   450  of    911.\n",
            "  Batch   500  of    911.\n",
            "  Batch   550  of    911.\n",
            "  Batch   600  of    911.\n",
            "  Batch   650  of    911.\n",
            "  Batch   700  of    911.\n",
            "  Batch   750  of    911.\n",
            "  Batch   800  of    911.\n",
            "  Batch   850  of    911.\n",
            "  Batch   900  of    911.\n",
            "\n",
            "Evaluating...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-274-c5138ddf6b25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#save the best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-273-94757a70940c>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;31m# model predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m       \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0;31m# compute the validation loss between actual and predicted values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-264-3d2c734195e3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sent_id, mask)\u001b[0m\n\u001b[1;32m     31\u001b[0m       \u001b[0;31m# print(f'bert shape: {preds.shape}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0;31m# x = self.fc1(cls_hs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x478 and 512x512)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kwsvr6G8wODy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}